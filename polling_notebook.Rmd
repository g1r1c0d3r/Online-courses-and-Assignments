---
title: "Logistic Regression model for predicting results of presidential elections"
output: html_notebook
---
I have published this on Rpubs which is more reader friendly- [Rpubs](http://rpubs.com/Nutan_Sahoo/predicting_the_unpredictable)
We see there are only 145 obsv. even though there are 50 states and 3 election years. so let's look at the no. of obsv per year using `table` function. In 2012 only 45 obsv are there. Actually, pollsters were so sure about these five states that they did not conduct any polls there.
```{r}
#description of variables
names(polling) 
```
Represents polling data in months leading up to 2004, 2008, 2012 pres. election. Each row represents a state in a particlular year.
`Republican`--- 1 if republican win and 0 if democrat win.
`Rasmussen`--- % of voters likely to vote for republican minus % who were likely to vote democrat.
`DiffCount`--- no. of polls that predicted a republican winner minus all polls that predicted a democrat
`PropR`--- proportion of all polls that predicted a republican winner.

```{r}
polling<- read.csv("polldata.csv")
#structure
str(polling)

#table of polling year
table(polling$Year)

#summary of polling  
summary(polling)

```

###Handling missing data
One method is that we delete observations, but here we already have less number of obsv. We won't prefer this. We can delete var with missing data ut we want to retain `Rasmussen/SurveyUSA`. Or filling them with average values. 
There is a bit more complicaed method called multiple imputation in which we fill missing values based on non-missing ones.It is mathematically sophisticated approach. We can do this easily through R's package called `mice` (Multiple Imputation through Chained Equations).

```{r,message=FALSE, warning=FALSE, eval=FALSE}
install.packages("mice") 
library("mice")
```
So for our multiple imputation to be useful we have to be able to find out the  values of our missing variables without using the outcome of the Republican. 

```{r}
simple<- polling[ ,c("Rasmussen", "SurveyUSA","PropR", "DiffCount")]
summary(simple)

```

```{r}
set.seed(144)
imputed<- complete(mice(simple))
summary(imputed)
polling$Rasmussen<- imputed$Rasmussen
polling$SurveyUSA<- imputed$SurveyUSA
```
Rasmussen and SurveyUSA have no more NA's
```{r}
polling$Rasmussen<- imputed$Rasmussen
polling$SurveyUSA<- imputed$SurveyUSA

```

###Sophisticated baseline model
```{r}
#splitting data into train and test set randomly
train<- subset(polling, Year==2004|Year==2008)
test<- subset(polling, Year == 2012)
table(train$Republican)

```
Since, the Republican won most of the state, our baseline model is aways going to predict Republican. So, we will have an accuracy of 53%. This is a pretty weak model. a smarter baseline model against which we can compare our model. We use func. sign here. If it's passed a negative number, it returns -1, if 0, then 0; If we passed the rasmussen variable into sign, whenever the republican was winning the state, it's gonna return 1.


```{r}

table(sign(train$Rasmussen))
```
-1 indicates democrat won and +1 indicates a republican. 

```{r}
#comparison of smart baseline model with basic baseline model
table(train$Republican, sign(train$Rasmussen))
```
0  and 1 in rows indicate democrat and republican win. 42 obsv where smart baseline correctly predicted that the democrat would win. This a better baseline model against which we can compare logistic approach.
We need to check for multicollinearity before we proceed---
```{r}
(cor(train[c("Rasmussen","SurveyUSA","PropR","DiffCount","Republican")]))
```
We see many ind. variables are highly correlated with one another. Let's start with one variable, it should be the one that's highly correlated i.e. `PropR` (0.94). 

###Fitting a model
```{r}
mod1<- glm(Republican ~ PropR, data = train, family = "binomial")
summary(mod1)

#predictions on train data
pred1<- predict(mod1, type = "response")
table(train$Republican, pred1>=0.5)
```
0 and 1 in rows indicate that a democrat or a republican won respt. True means we predicted republican and false means democrat. We can see we correctly predicted for 45 democrats and 51 republican. It makes 4 mistakes.


We will see if we can improve the predictions, we will select a var which is less related to PropR. It is less correlated to surveyusa and diffcount, so we try them out.
```{r}
mod2<- glm(Republican ~ SurveyUSA+DiffCount , data = train, family = "binomial")
pred2<- predict(mod2, type="response")
table(train$Republican, pred2>=0.5)
summary(mod2)

```
We make one less mistake but nothing too impressive here and neither of the variables are significant.

###Test set predictions
first we will use smart baseline model to predict the outcome of the 
```{r}
table(test$Republican, sign(test$Rasmussen)) #using smart baseline for predictions

testpred<- predict(mod2, newdata= test , type="response")
table(test$Republican, testpred>=0.5)

```
Smart baseline correctly predicted election results for 18 democrat and 21 republican. It makes 4 mistakes and 2 were inconclusive. 
Our model predicts correctly for 44 observations out of 45 and 1 was incorrect.
Let's look at the mistake we have made here---
```{r}
subset(test, testpred>=0.5 & Republican==0)
```
Here, Rasmussen is 2, DiffCount is 6, it points towards republican winning the election. But in reality Barack obama won the state of florida in 2012 and he is from democrat. However overall it outperforms baseline model. Hence, it is a good model.

Here we are fine by just using a cutoff of 0.5 as we are not much concerned with errors and we are trying to predict for different states a binary outcome. so we won't use ROC curve.








